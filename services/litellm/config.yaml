# LiteLLM Proxy Configuration
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# WHY: Central LLM gateway — all apps call this instead of providers directly.
#      Gives us: routing, fallbacks, cost tracking, rate limiting, observability.
#
# HOW: Deployed as Docker container on Railway. Apps use OpenAI-compatible API.

model_list:
  # ─── Anthropic ────────────────────────────────────────────────────
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: claude-haiku
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # ─── OpenAI ───────────────────────────────────────────────────────
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: "os.environ/OPENAI_API_KEY"

  # ─── OpenRouter (Mercury) ─────────────────────────────────────────
  - model_name: mercury-coder
    litellm_params:
      model: openrouter/inception/mercury-coder-small-beta
      api_key: "os.environ/OPENROUTER_API_KEY"

  # ─── Embedding (§19 INPUT — pgvector requires consistent model_id) ───
  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: "os.environ/OPENAI_API_KEY"

  # ─── LlamaGuard (Output Guardrails, §21) ────────────────────
  - model_name: llamaguard
    litellm_params:
      model: together_ai/meta-llama/Llama-Guard-4-12B
      api_key: "os.environ/TOGETHER_AI_API_KEY"

# ─── LiteLLM settings ──────────────────────────────────────────────
# Langfuse callbacks activate when LANGFUSE_PUBLIC_KEY + LANGFUSE_SECRET_KEY
# are set on Railway. No-op otherwise (LiteLLM ignores unconfigured callbacks).
litellm_settings:
  drop_params: true
  cache: true
  cache_params:
    type: "redis"
    host: "os.environ/REDIS_HOST"
    port: "os.environ/REDIS_PORT"
    password: "os.environ/REDIS_PASSWORD"
    ttl: 3600  # 1h default — conversational. Override per-route for factual (86400). §18 Semantic Caching.
  allow_requests_on_db_unavailable: true
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

# ─── Router settings (§18 RouteLLM — cost-optimized model routing) ───
# Maps tier aliases from packages/shared-llm/src/routing/router.ts
# to model groups. LiteLLM uses lowest-cost routing within each group.
router_settings:
  routing_strategy: "cost-based-routing"
  model_group_alias:
    fast-tier: "gpt-4o-mini"
    balanced-tier: "claude-haiku"
    quality-tier: "claude-sonnet"
  num_retries: 2
  retry_after: 5
  allowed_fails: 3
  cooldown_time: 60

# ─── General settings ───────────────────────────────────────────────
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  store_model_in_db: false
