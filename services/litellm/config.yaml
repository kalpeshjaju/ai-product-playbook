# LiteLLM Proxy Configuration
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# WHY: Central LLM gateway — all apps call this instead of providers directly.
#      Gives us: routing, fallbacks, cost tracking, rate limiting, observability.
#
# HOW: Deployed as Docker container on Railway. Apps use OpenAI-compatible API.

model_list:
  # ─── Anthropic ────────────────────────────────────────────────────
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: claude-haiku
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # ─── OpenAI ───────────────────────────────────────────────────────
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: "os.environ/OPENAI_API_KEY"

  # ─── OpenRouter (Mercury) ─────────────────────────────────────
  - model_name: mercury-coder
    litellm_params:
      model: openrouter/inception/mercury-coder-small-beta
      api_key: "os.environ/OPENROUTER_API_KEY"

# ─── LiteLLM settings ──────────────────────────────────────────────
# Langfuse callbacks activate when LANGFUSE_PUBLIC_KEY + LANGFUSE_SECRET_KEY
# are set on Railway. No-op otherwise (LiteLLM ignores unconfigured callbacks).
litellm_settings:
  drop_params: true
  cache: true
  cache_params:
    type: "redis"
    host: "os.environ/REDIS_HOST"
    port: "os.environ/REDIS_PORT"
    password: "os.environ/REDIS_PASSWORD"
    ttl: 3600  # 1h default — conversational. Override per-route for factual (86400). §18 Semantic Caching.
  allow_requests_on_db_unavailable: true
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

# ─── General settings ───────────────────────────────────────────────
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  store_model_in_db: false
