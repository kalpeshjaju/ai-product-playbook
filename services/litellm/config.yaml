# LiteLLM Proxy Configuration
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# WHY: Central LLM gateway — all apps call this instead of providers directly.
#      Gives us: routing, fallbacks, cost tracking, rate limiting, observability.
#
# HOW: Deployed as Docker container on Railway. Apps use OpenAI-compatible API.

model_list:
  # ─── Anthropic ────────────────────────────────────────────────────
  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: claude-haiku
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # ─── OpenAI ───────────────────────────────────────────────────────
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: "os.environ/OPENAI_API_KEY"

# ─── Callbacks (Langfuse for observability) ─────────────────────────
litellm_settings:
  drop_params: true
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  cache: false

# ─── General settings ───────────────────────────────────────────────
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"
  store_model_in_db: false
